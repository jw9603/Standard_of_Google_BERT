{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOifeGK96WwFc8q3Evkiap3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## HuggingFace Transformers\n","\n","Hugging Face is an organization that is on a path to solve and democratize AI through natural language. Their open-source library 'transformers' is very popular among the NLP community. It is very useful and powerful for several NLP and NLU tasks. It includes thousands of pre-trained models in about 100+ languages. One of the many advantages of the transformer library is that it is compatible with both PyTorch and TensorFlow.\n","\n","We can install transformers directly using pip as shown in the following:"],"metadata":{"id":"MHV0Aha6OhL0"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qshjq-X4OcG2","executionInfo":{"status":"ok","timestamp":1699938547556,"user_tz":-540,"elapsed":16870,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"83d31d8c-7727-4d7f-b2bd-6ebaf2295e56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["### Generating Bert Embedding\n","\n","In this section, we will learn how to extract embeddings from the pre-trained BERT.\n","Consider the sentence 'I love Paris'. Let's see how to obtain the contextualized word embedding of all the words in the sentence using the pre trained BERT model with Huggung Face's transformer library.\n","\n","First, let's import the necessary modules:"],"metadata":{"id":"Wq39dWghPCQA"}},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","import torch"],"metadata":{"id":"2isNa0o7OrsP","executionInfo":{"status":"ok","timestamp":1699940025237,"user_tz":-540,"elapsed":770,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Next, we download the pretraned BERT model. We can check all the available pretrained BERT models here : https://huggingface.co/docs/transformers/index\n","\n","We use the 'bert-base-uncased' model. As the name suggests, it is the BERT-based model with 12 encoders and it is trained with uncased tokens.\n","\n","Since we are using the BERT-base, the representation size will be 768.\n","\n","Download and load the pre-trained bert model :"],"metadata":{"id":"AlelQIdbUnDd"}},{"cell_type":"code","source":["model = BertModel.from_pretrained('bert-base-uncased')\n"],"metadata":{"id":"R40izMOFUjYq","executionInfo":{"status":"ok","timestamp":1699941747140,"user_tz":-540,"elapsed":2760,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DFb4SYVVRog","executionInfo":{"status":"ok","timestamp":1699941691540,"user_tz":-540,"elapsed":745,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"51176072-889b-4d8b-b897-e0ddcbc1d781"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### Preprocessing the input"],"metadata":{"id":"qwEqD2R_YR8C"}},{"cell_type":"markdown","source":["Define the sentence:"],"metadata":{"id":"kx6zYZw7YUDc"}},{"cell_type":"code","source":["s = 'I love Paris'"],"metadata":{"id":"wGBYVKDmVvBF","executionInfo":{"status":"ok","timestamp":1699941692761,"user_tz":-540,"elapsed":1,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["Tokenize the sentence and obtain the tokens:"],"metadata":{"id":"mIA_KOS7Ybrd"}},{"cell_type":"code","source":["tokens = tokenizer.tokenize(s)\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCwQTAOEYbJT","executionInfo":{"status":"ok","timestamp":1699941693424,"user_tz":-540,"elapsed":5,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"aff9ec88-a1da-4616-a42a-192e69f628dc"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'love', 'paris']"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Now, we will add the [CLS] token at the beginning and [SEP] token at the end of the tokens list:"],"metadata":{"id":"EE6Q-Zo2Yh0L"}},{"cell_type":"code","source":["tokens = ['[CLS]'] + tokens + ['[SEP]']\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7L_TRaxYgXy","executionInfo":{"status":"ok","timestamp":1699941693424,"user_tz":-540,"elapsed":4,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"f6ae04de-c1c2-4863-cbf7-655b79e84850"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'i', 'love', 'paris', '[SEP]']"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["As we can observe, we have [CLS] token at the begining and sep token at the end of our tokens list. we can also observe that length of our tokens is 5.\n","\n","Say, we need to keep the length of our tokens list to 7, in that case, we will add two [PAD] tokens at the end as show in the following:"],"metadata":{"id":"Qxd1qfkSYumI"}},{"cell_type":"code","source":["tokens += ['[PAD]']*2\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndskib9yYtBc","executionInfo":{"status":"ok","timestamp":1699941693425,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"a852742a-ff5f-4472-e1e7-c1db6356a023"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Next, we create the attention mask. We set the attention mask value to 1 if the token is not a [PAD] token else we will set the attention mask to 0 as shown below:"],"metadata":{"id":"J42uwAcfZJcY"}},{"cell_type":"code","source":["attention_mask = [1 if t!= '[PAD]' else 0 for t in tokens]"],"metadata":{"id":"zWcwVMM2ZDzH","executionInfo":{"status":"ok","timestamp":1699941695512,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["attention_mask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUfSFjhDZbwA","executionInfo":{"status":"ok","timestamp":1699941695512,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"f31ff3cd-ffc7-480b-da03-96b33f2e5707"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1, 1, 0, 0]"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["we convert all the tokens to their token_ids as shown below:"],"metadata":{"id":"9aQePxs_ZhvD"}},{"cell_type":"code","source":["token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","token_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KDC4n38YZgDA","executionInfo":{"status":"ok","timestamp":1699941697547,"user_tz":-540,"elapsed":5,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"3c1445fa-a798-450a-adc0-29a4f2f12d3a"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 1045, 2293, 3000, 102, 0, 0]"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["we convert the token_ids and attention_mask to tensors:"],"metadata":{"id":"zOCIkNTEZ2wG"}},{"cell_type":"code","source":["token_ids = torch.tensor(token_ids).unsqueeze(0)\n","attention_mask = torch.tensor(attention_mask).unsqueeze(0)"],"metadata":{"id":"vkdeOZ7fZpZ9","executionInfo":{"status":"ok","timestamp":1699941698310,"user_tz":-540,"elapsed":8,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["print(token_ids)\n","print(attention_mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vwuIdLlZ_oo","executionInfo":{"status":"ok","timestamp":1699941698310,"user_tz":-540,"elapsed":7,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"e551ea40-84a6-4daa-a937-5032185b4bec"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 101, 1045, 2293, 3000,  102,    0,    0]])\n","tensor([[1, 1, 1, 1, 1, 0, 0]])\n"]}]},{"cell_type":"markdown","source":["### Getting the embedding\n","\n","As shown in the following code, we feed the token_ids, and attention_mask to the model and get the embeddings. Note that the model returns the output as a tuple with two values.\n","\n","The first value indicates the hidden state representation, hidden_rep and it consists of the representation of all the tokens obtained from the final encoder (encoder 12), and the second value, cls_head consists of the representation of the [CLS] token:"],"metadata":{"id":"WbUc2GaBaEsB"}},{"cell_type":"code","source":["output=model(token_ids,attention_mask=attention_mask)\n","output.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VS-fbZeaB-F","executionInfo":{"status":"ok","timestamp":1699942088322,"user_tz":-540,"elapsed":333,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"94432cc4-2f9c-49f9-9ab3-97d52070b3f2"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output'])"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["각 값들의 의미를 살펴보자.\n","\n","우선 last_hidden_state는 마지막 layer의 hidden state이다. bert-base-uncased의 경우 (batch_size, sequence_length, 768) 크기의 tensor이다. 일반적으로 이 값을 입력된 텍스트에 대해 BERT가 생성한 최종 embedding으로 여긴다. 이 embedding을 사용하여 downstream task를 수행한다."],"metadata":{"id":"6eZBHyfkcqqz"}},{"cell_type":"code","source":["print(output[0]==output.last_hidden_state) # hls_rep\n","print(output[1].shape) # cls_head에 linear+activate func(tanH)를 거친 값"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ng6obfbatjP","executionInfo":{"status":"ok","timestamp":1699942130782,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"27929871-e51b-4311-e253-950447f4a086"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True],\n","         ...,\n","         [True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True]]])\n","torch.Size([1, 768])\n"]}]},{"cell_type":"code","source":["output[0][0][0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwM6aoMnayAG","executionInfo":{"status":"ok","timestamp":1699941954106,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"afb6591c-bf07-44af-bf14-eb2529ac2727"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([768])"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["output[1][0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0p7FQIcbjOz","executionInfo":{"status":"ok","timestamp":1699941963699,"user_tz":-540,"elapsed":4,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"7a8aeafa-3933-4b41-93ad-b40d4586728a"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([768])"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["from transformers import BertModel\n","\n","model1 = BertModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False, output_hidden_states=True, output_attentions=True)"],"metadata":{"id":"6ODXxpo0dmJF","executionInfo":{"status":"ok","timestamp":1699942426168,"user_tz":-540,"elapsed":2638,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["output=model1(token_ids,attention_mask=attention_mask)\n","output.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53bHv2n9b993","executionInfo":{"status":"ok","timestamp":1699942426682,"user_tz":-540,"elapsed":516,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"fd61e4b7-2bcd-4f09-f175-4ba3d3531c40"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["hidden_states는 각 layer의 hidden state를 모아놓은 list이다. 이때 마지막 layer일수록 뒤에 있다. 즉 hidden_states[-1]과 last_hidden_state는 같다. bert-base-uncased의 경우 길이 13인 list이고(첫 번째 원소는 BertEmbeddings 모듈의 출력값이다), 각 원소는 크기 (batch_size, sequence_length, 768)인 tensor이다."],"metadata":{"id":"GIP0li_reaKw"}},{"cell_type":"code","source":["print(output[0].shape)\n","print(output.hidden_states[0].shape)\n","print(type(output[2]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpzDedjBdtkZ","executionInfo":{"status":"ok","timestamp":1699942617053,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"9182a17f-5804-4c88-fc7b-fc1a6ee5ed50"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 7, 768])\n","torch.Size([1, 7, 768])\n","<class 'tuple'>\n"]}]},{"cell_type":"code","source":["output[0]==output.hidden_states[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5_bZBNCd0Ky","executionInfo":{"status":"ok","timestamp":1699942636555,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"8e4e46bb-54af-41aa-a28e-653ec741a504"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True],\n","         ...,\n","         [True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True],\n","         [True, True, True,  ..., True, True, True]]])"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["attentions은 각 layer의 attention weight를 모아놓은 list이다. 이때 마지막 layer일수록 뒤에 있다. bert-base-uncased의 경우 길이 12인 list이고, 각 원소는 크기 (batch_size, 12, sequence_length, sequence_length)인 tensor이다."],"metadata":{"id":"3qTS15UFekT8"}},{"cell_type":"code","source":["print(len(output.attentions))\n","print(output.attentions[0].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypsewhHUeft9","executionInfo":{"status":"ok","timestamp":1699942732085,"user_tz":-540,"elapsed":288,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"d23d853c-1e62-40e3-97e5-015b6a9790b5"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["12\n","torch.Size([1, 12, 7, 7])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dfcToGG-etoG"},"execution_count":null,"outputs":[]}]}