# Standard_of_Google_BERT
This is a repository for recording while studying the book "The Standard of Google BERT".

### About this Book

Transformer's Bidirectional Encoder Representation (BERT) revolutionized the world of natural language processing with good performance. This book is a guide that will help you understand the Google BERT architecture. Starting with a detailed explanation of the transformer architecture, work your way through the introduction to learn how the transformer's encoders and decoders work.

Based on our understanding of BERT, we will look at its architecture, how the BERT model is pre-trained, and how to fine-tune BERT and use it for downstream tasks.
Subsequently, various variants of BERT such as ALBERT, RoBERTa, ELECTRA, and SpanBERT are introduced, and variants of BERT based on knowledge distillation such as DistilBERT and TinyBERT are explored.


It also explains M-BERT, XLM, and XLM-R in detail. Next, we learn about sentence-BERT, which is used to obtain sentence representations. It also identifies what BERT models correspond to specific domains, such as BioBERT and ClinicalBERT. Finally, you will learn about an interesting variant of BERT called VideoBERT.

By the end of this book, you will be using BERT and its variants to perform several natural language processing tasks.

### Goal

**Even if I can't do it every day, I'm going to keep recording. Even 30 minutes a day!!!**

### Composition of the book

[PART I BERT 시작하기]

CHAPTER 1 트랜스포머 입문

1.1 트랜스포머 소개

1.2 트랜스포머의 인코더 이해하기

1.3 트랜스포머 디코더 이해하기

1.4 인코더와 디코더 결합

1.5 트랜스포머 학습

1.6 마치며

1.7 연습 문제

1.8 보충 자료

CHAPTER 2 BERT 이해하기

2.1 BERT 기본 개념

2.2 BERT의 동작 방식

2.3 BERT의 구조

2.4 BERT 사전 학습

2.5 하위 단위 토큰화 알고리즘

2.6 마치며

2.7 연습 문제

2.8 보충 자료

CHAPTER 3 BERT 활용하기

3.1 사전 학습된 BERT 모델 탐색

3.2 사전 학습된 BERT에서 임베딩을 추출하는 방법

3.3 BERT의 모든 인코더 레이어에서 임베딩을 추출하는 방법

3.4 다운스트림 태스크를 위한 BERT 파인 튜닝 방법

3.5 마치며

3.6 연습 문제

3.7 보충 자료

[PART II BERT 파생 모델]

CHAPTER 4 B ERT의 파생 모델 I: ALBERT, RoBERTa, ELECTRA, SpanBERT

4.1 ALBERT

4.2 ALBERT에서 임베딩 추출

4.3 RoBERTa

4.4 ELECTRA 이해하기

4.5 SpanBERT로 스팬 예측

4.6 마치며

4.7 연습 문제

4.8 보충 자료

CHAPTER 5 BERT 파생 모델 II: 지식 증류 기반

5.1 지식 증류 소개

5.2 DistilBERT: BERT의 지식 증류 버전

5.3 TinyBERT 소개

5.4 BERT에서 신경망으로 지식 전달

5.5 마치며

5.6 연습 문제

5.7 보충 자료

[PART III BERT 적용하기]

CHAPTER 6 텍스트 요약을 위한 BERTSUM 탐색

6.1 텍스트 요약

6.2 텍스트 요약에 맞춘 BERT 파인 튜닝

6.3 ROUGE 평가 지표 이해하기

6.4 BERTSUM 모델의 성능

6.5 BERTSUM 모델 학습

6.6 마치며

6.7 연습 문제

6.8 보충 자료

CHAPTER 7 다른 언어에 BERT 적용하기

7.1 M-BERT 이해하기

7.2 M-BERT는 다국어 표현이 어떻게 가능한가?

7.3 XLM

7.4 XLM-R 이해하기

7.5 언어별 BERT

7.6 마치며

7.7 연습 문제

7.8 보충 자료

CHAPTER 8 sentence-BERT 및 domain-BERT 살펴보기

8.1 sentence-BERT로 문장 표현 배우기

8.2 sentence-transformers 라이브러리 탐색

8.3 지식 증류를 이용한 다국어 임베딩 학습

8.4 domain-BERT

8.5 마치며

8.6 연습 문제

8.7 보충 자료

CHAPTER 9 VideoBERT, BART

9.1 VideoBERT로 언어 및 비디오 표현 학습

9.2 BART 이해하기

9.3 BERT 라이브러리 탐색

9.4 마치며

9.5 연습 문제

9.6 보충 자료

CHAPTER 10 한국어 언어 모델: KoBERT, KoGPT2, KoBART

10.1 KoBERT

10.2 KoGPT2

10.3 KoBART
